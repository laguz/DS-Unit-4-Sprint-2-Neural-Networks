{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forgot to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAO7xRIfVhNV",
        "outputId": "7efa99ff-6556-43c0-a80b-7d6c43ff7572"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6BMy5LFU9o6"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from keras.activations import relu, sigmoid\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbCrl2-aU9o7"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK9BqtIaU9o8"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngmAJ8GmU9o8",
        "outputId": "a07d54a4-d271-452e-eaf5-6cc5a253c51f"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CfgghKuU9o8",
        "outputId": "fc6e5b0e-9e83-48ac-eed1-4636ed3c0060"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TgGwS0xU9o8"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D1fgwR9U9o_"
      },
      "source": [
        "## Explore create_model\n",
        "\n",
        "Let's build a few different models in order to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92yQfihmU9o_"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dkfkZWpdU9o_"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(10, 500, 100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDSrtboVU9pA",
        "outputId": "4d4d31e5-7a17-43f6-8c08-5482cd368506"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6YqJJ9U9pA"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WumhjLaHU9pB"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(10, 500, 100, negative_node_incrementation = False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3trpNkeU9pB",
        "outputId": "52f43a6b-532a-4aed-b93d-4add249d7c1f"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 545)               273045    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 589)               321594    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 634)               374060    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 678)               430530    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 723)               490917    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 767)               555308    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 812)               623616    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 856)               695928    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                8570      \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RTVgf4OU9pB"
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIAcvwLU9pB"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z15Bg73YU9pB"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "AzhaPZOrU9pC"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(2, 500, 100)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S721ELNU9pC",
        "outputId": "33fd77c5-2149-40d3-849f-88855de98e98"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vvxx1FKU9pC"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53wlCGi4U9pC"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86UKoMHU9pC",
        "outputId": "16225d0e-1e0b-490f-8fd5-085d7cecf88b"
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 19s 4ms/step - loss: 0.8219 - accuracy: 0.7514\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4528 - accuracy: 0.8670\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3600 - accuracy: 0.8912\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4569 - accuracy: 0.8638\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8184 - accuracy: 0.7506\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4372 - accuracy: 0.8684\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3470 - accuracy: 0.8953\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4844 - accuracy: 0.8621\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8396 - accuracy: 0.7458\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4579 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3612 - accuracy: 0.8932\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4683 - accuracy: 0.8655\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7962 - accuracy: 0.7544\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4338 - accuracy: 0.8681\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3466 - accuracy: 0.8935\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4364 - accuracy: 0.8679\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7913 - accuracy: 0.7531\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4236 - accuracy: 0.8688\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3231 - accuracy: 0.8987\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4757 - accuracy: 0.8626\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7963 - accuracy: 0.7517\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.4343 - accuracy: 0.8675\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.3305 - accuracy: 0.9004\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4445 - accuracy: 0.8716\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8282 - accuracy: 0.7504\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4502 - accuracy: 0.8654\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3603 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4498 - accuracy: 0.8678\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8163 - accuracy: 0.7528\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4474 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3491 - accuracy: 0.8948\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4690 - accuracy: 0.8657\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8331 - accuracy: 0.7450\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4587 - accuracy: 0.8637\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3542 - accuracy: 0.8934\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4608 - accuracy: 0.8674\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7981 - accuracy: 0.7523\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.4329 - accuracy: 0.8694\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3362 - accuracy: 0.8965\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4479 - accuracy: 0.8675\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7934 - accuracy: 0.7522\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.4224 - accuracy: 0.8716\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.3318 - accuracy: 0.8979\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4734 - accuracy: 0.8626\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8017 - accuracy: 0.7500\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4269 - accuracy: 0.8695\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3374 - accuracy: 0.8969\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4645 - accuracy: 0.8676\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8525 - accuracy: 0.7453\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4781 - accuracy: 0.8570\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3871 - accuracy: 0.8869\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.4706 - accuracy: 0.8601\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8512 - accuracy: 0.7367\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4722 - accuracy: 0.8597\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3730 - accuracy: 0.8889\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.4791 - accuracy: 0.8613\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8699 - accuracy: 0.7346\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4731 - accuracy: 0.8594\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3814 - accuracy: 0.8873\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.4638 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8301 - accuracy: 0.7454\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4543 - accuracy: 0.8607\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3650 - accuracy: 0.8884\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4791 - accuracy: 0.8560\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8204 - accuracy: 0.7442\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4384 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3480 - accuracy: 0.8927\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4626 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8324 - accuracy: 0.7408\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4471 - accuracy: 0.8630\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3531 - accuracy: 0.8924\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4586 - accuracy: 0.8656\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8606 - accuracy: 0.7416\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4887 - accuracy: 0.8573\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3899 - accuracy: 0.8840\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4562 - accuracy: 0.8629\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8498 - accuracy: 0.7407\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4732 - accuracy: 0.8599\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3724 - accuracy: 0.8890\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5028 - accuracy: 0.8525\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8598 - accuracy: 0.7398\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4749 - accuracy: 0.8569\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3870 - accuracy: 0.8843\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.4675 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8442 - accuracy: 0.7389\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.4508 - accuracy: 0.8647\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.3528 - accuracy: 0.8928\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4546 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8317 - accuracy: 0.7376\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4508 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3499 - accuracy: 0.8926\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4713 - accuracy: 0.8607\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8357 - accuracy: 0.7420\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4526 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3559 - accuracy: 0.8913\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.8633\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  9.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.7307 - accuracy: 0.7724\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: 0.4084 - accuracy: 0.8742\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: 0.3187 - accuracy: 0.9014\n",
            "Best: 0.867359975973765 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.863813320795695, Stdev: 0.0013880875276375197 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.867359975973765, Stdev: 0.0037117467434549384 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8669733206431071, Stdev: 0.0009011694746335795 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8658933242162069, Stdev: 0.0023579397586470387 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8620666662851969, Stdev: 0.002021291128872522 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8622533281644186, Stdev: 0.004399429705841283 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.860813319683075, Stdev: 0.006105236719272566 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8629199862480164, Stdev: 0.0016688543399030488 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5uYgq-9U9pD"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHWfTDQAU9pD",
        "outputId": "840c33e2-4177-4d8c-95c3-398f84ff226e"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNUCy6YrU9pD"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQs66_1sU9pD"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0eXVkuuU9pE"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OypN7M-zU9pE",
        "outputId": "deece4d7-b21f-4619-af9b-7bf32977af01"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIY_TY-CU9pE"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7hWVeNzU9pE",
        "outputId": "98fb9bc7-bcd9-4b6d-f10d-f3477d889baa"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "units_n = 512 / 32\n",
        "learning_n = 3\n",
        "activation_n = 2\n",
        "\n",
        "n_unique_hparam_combos = units_n * learning_n * activation_n\n",
        "n_unique_hparam_combos"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf8-nCT7U9pF",
        "outputId": "026b686f-0ac8-4431-dccc-c8fb29cdd7a2"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "n_param_combos_to_sample = n_unique_hparam_combos * .25\n",
        "n_param_combos_to_sample = int(n_param_combos_to_sample)\n",
        "n_param_combos_to_sample"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PflqFwmlU9pF",
        "outputId": "00747c8e-9cab-444c-afa3-18ec061c25b7"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./keras-tuner-trial/random_search/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from ./keras-tuner-trial/random_search/tuner0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDEilBvuU9pF",
        "outputId": "a746ee41-4d60-420c-f14f-ed44653815d6"
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 13 Complete [00h 00m 21s]\n",
            "val_accuracy: 0.6828399896621704\n",
            "\n",
            "Best val_accuracy So Far: 0.8779199719429016\n",
            "Total elapsed time: 00h 06m 02s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GsRw3OKU9pG",
        "outputId": "9fc33a9d-cd72-4445-b528-b0f5bb4e8624"
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8779199719429016\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.870639979839325\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.864799976348877\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8645600080490112\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8614799976348877\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.858959972858429\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8588399887084961\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8359599709510803\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8335599899291992\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8310800194740295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "8ztBy8ZGU9pG"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "D1qVnJweU9pG"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-FTaBg-U9pG"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFH_2NUIU9pH"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmi7gvgU9pH"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sxs1JSgU9pH",
        "outputId": "34290f5d-63c0-46c9-ba91-f3dd922820ff"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 00m 14s]\n",
            "val_accuracy: 0.828279972076416\n",
            "\n",
            "Best val_accuracy So Far: 0.8752400279045105\n",
            "Total elapsed time: 00h 06m 53s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNpEQxF-U9pH",
        "outputId": "c5123409-54bf-488b-85b7-48cc330b0923"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8752400279045105\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8664399981498718\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8621199727058411\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8567600250244141\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8464000225067139\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.828279972076416\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8226400017738342\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8205999732017517\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8162000179290771\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8095999956130981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIGowwL9U9pH"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "iLo0-IDZU9pH"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCQ4Ju9U9pH"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g965WQyUU9pI"
      },
      "source": [
        "### Populate a Sklearn compatiable parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yn2GuFGU9pI"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaCnagaXU9pI",
        "outputId": "8e0eb7a7-2970-47e2-d2de-2a2cce08d26e"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSeIUjD3U9pI"
      },
      "source": [
        "### Build a Sklearn compatiable model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Q2RHicU9pI"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHctu94pU9pI"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxkvfqOjU9pI",
        "outputId": "6a0dea1f-02ee-4310-c154-cf0cc57a32e4"
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 10s 1ms/step - loss: 1.8585 - accuracy: 0.3349\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7948 - accuracy: 0.3223\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 2.0218 - accuracy: 0.3015\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9970 - accuracy: 0.2701\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.9213 - accuracy: 0.3337\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7908 - accuracy: 0.3317\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9348 - accuracy: 0.3855\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8684 - accuracy: 0.3177\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2543 - accuracy: 0.2780\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9987 - accuracy: 0.2636\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0762 - accuracy: 0.3427\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7582 - accuracy: 0.3343\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2545 - accuracy: 0.2728\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9451 - accuracy: 0.2528\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1292 - accuracy: 0.3376\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8920 - accuracy: 0.2821\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0622 - accuracy: 0.3417\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8959 - accuracy: 0.2887\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2532 - accuracy: 0.3564\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9564 - accuracy: 0.2528\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.4446 - accuracy: 0.3152\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9584 - accuracy: 0.2786\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.4164 - accuracy: 0.2666\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9417 - accuracy: 0.2472\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1850 - accuracy: 0.3601\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9513 - accuracy: 0.2989\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5575 - accuracy: 0.2750\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9594 - accuracy: 0.2462\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.3064 - accuracy: 0.4093\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0469 - accuracy: 0.2855\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.6799 - accuracy: 0.3411\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8622 - accuracy: 0.3172\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5165 - accuracy: 0.3809\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9238 - accuracy: 0.3052\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 2.3176 - accuracy: 0.3402\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9150 - accuracy: 0.2749\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.7686 - accuracy: 0.2835\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9725 - accuracy: 0.2652\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5842 - accuracy: 0.3561\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1108 - accuracy: 0.3128\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5377 - accuracy: 0.3203\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8364 - accuracy: 0.3062\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.0723 - accuracy: 0.3054\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.2542 - accuracy: 0.1867\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.5413 - accuracy: 0.3101\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9909 - accuracy: 0.2405\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.6752 - accuracy: 0.3532\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8632 - accuracy: 0.2925\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.6429 - accuracy: 0.3632\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8535 - accuracy: 0.3206\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7762 - accuracy: 0.3387\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8652 - accuracy: 0.2813\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.0028 - accuracy: 0.3152\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1248 - accuracy: 0.2667\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7474 - accuracy: 0.3265\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8910 - accuracy: 0.3087\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.2651 - accuracy: 0.3347\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0034 - accuracy: 0.2739\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.2194 - accuracy: 0.3690\n",
            "782/782 [==============================] - 2s 1ms/step - loss: 2.1080 - accuracy: 0.3332\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.9730 - accuracy: 0.3140\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8556 - accuracy: 0.3248\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.9000 - accuracy: 0.3286\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9161 - accuracy: 0.2624\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.9350 - accuracy: 0.3579\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8878 - accuracy: 0.2800\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.7119 - accuracy: 0.3269\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0047 - accuracy: 0.2602\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.8105 - accuracy: 0.3423\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9453 - accuracy: 0.2858\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.1148 - accuracy: 0.3682\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0708 - accuracy: 0.3498\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 4.2604 - accuracy: 0.3702\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9553 - accuracy: 0.3292\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.4771 - accuracy: 0.3566\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8713 - accuracy: 0.3422\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.2184 - accuracy: 0.3433\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7585 - accuracy: 0.3304\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.9093 - accuracy: 0.3449\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0914 - accuracy: 0.2578\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.0235 - accuracy: 0.4394\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8486 - accuracy: 0.4166\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.3276 - accuracy: 0.3626\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.5116 - accuracy: 0.3220\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3350 - accuracy: 0.3621\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0380 - accuracy: 0.2763\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 4.1383 - accuracy: 0.3080\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0947 - accuracy: 0.2505\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 4.0205 - accuracy: 0.4405\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7175 - accuracy: 0.4038\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.4323 - accuracy: 0.3950\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8845 - accuracy: 0.3250\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.4980 - accuracy: 0.3944\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9435 - accuracy: 0.3110\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.4035 - accuracy: 0.3959\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9296 - accuracy: 0.2605\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9265 - accuracy: 0.7153\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.7888\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9141 - accuracy: 0.7179\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7194 - accuracy: 0.7868\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9353 - accuracy: 0.7140\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6996 - accuracy: 0.7908\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8763 - accuracy: 0.7314\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6819 - accuracy: 0.7904\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8750 - accuracy: 0.7280\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.7990\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8803 - accuracy: 0.7279\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6841 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8649 - accuracy: 0.7345\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6765 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8527 - accuracy: 0.7388\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6854 - accuracy: 0.7997\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8723 - accuracy: 0.7278\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6736 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8495 - accuracy: 0.7406\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6466 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8530 - accuracy: 0.7372\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7089 - accuracy: 0.7952\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8643 - accuracy: 0.7325\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6510 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8593 - accuracy: 0.7325\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6644 - accuracy: 0.8055\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8509 - accuracy: 0.7431\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6688 - accuracy: 0.8079\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8685 - accuracy: 0.7351\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6745 - accuracy: 0.7988\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8583 - accuracy: 0.7377\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6666 - accuracy: 0.7961\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8390 - accuracy: 0.7444\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6750 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8494 - accuracy: 0.7369\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6609 - accuracy: 0.8023\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8819 - accuracy: 0.7356\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6458 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8535 - accuracy: 0.7363\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6865 - accuracy: 0.8015\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8482 - accuracy: 0.7388\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6573 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8612 - accuracy: 0.7373\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6419 - accuracy: 0.8076\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8659 - accuracy: 0.7384\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7056 - accuracy: 0.7890\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8724 - accuracy: 0.7332\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6147 - accuracy: 0.8190\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8682 - accuracy: 0.7350\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6669 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8410 - accuracy: 0.7416\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6862 - accuracy: 0.7966\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8516 - accuracy: 0.7398\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6563 - accuracy: 0.8079\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8774 - accuracy: 0.7328\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6518 - accuracy: 0.8061\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8581 - accuracy: 0.7382\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6739 - accuracy: 0.8089\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8680 - accuracy: 0.7353\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6546 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8598 - accuracy: 0.7349\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6880 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8589 - accuracy: 0.7330\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6494 - accuracy: 0.8104\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8582 - accuracy: 0.7337\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6327 - accuracy: 0.8146\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8683 - accuracy: 0.7397\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6564 - accuracy: 0.8029\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8724 - accuracy: 0.7384\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6922 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8563 - accuracy: 0.7421\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6644 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8717 - accuracy: 0.7387\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6724 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8640 - accuracy: 0.7371\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6757 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8661 - accuracy: 0.7397\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6697 - accuracy: 0.7961\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8608 - accuracy: 0.7372\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6560 - accuracy: 0.8031\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8965 - accuracy: 0.7346\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6754 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8741 - accuracy: 0.7344\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6572 - accuracy: 0.8070\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8541 - accuracy: 0.7431\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6342 - accuracy: 0.8133\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8716 - accuracy: 0.7386\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7277 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8803 - accuracy: 0.7349\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6563 - accuracy: 0.8057\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8735 - accuracy: 0.7354\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6904 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8823 - accuracy: 0.7354\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6901 - accuracy: 0.7993\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8829 - accuracy: 0.7337\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6520 - accuracy: 0.8124\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0857 - accuracy: 0.6675\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7103 - accuracy: 0.7899\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0519 - accuracy: 0.6822\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7178 - accuracy: 0.7922\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0606 - accuracy: 0.6779\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7030 - accuracy: 0.7936\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9780 - accuracy: 0.7004\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6128 - accuracy: 0.8180\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9923 - accuracy: 0.6945\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6547 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0156 - accuracy: 0.6877\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6501 - accuracy: 0.8139\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9639 - accuracy: 0.7062\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6009 - accuracy: 0.8208\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9638 - accuracy: 0.7020\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6470 - accuracy: 0.8152\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9524 - accuracy: 0.7111\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6128 - accuracy: 0.8225\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9249 - accuracy: 0.7190\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5849 - accuracy: 0.8256\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9133 - accuracy: 0.7256\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5972 - accuracy: 0.8252\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9349 - accuracy: 0.7160\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5988 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9096 - accuracy: 0.7245\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5701 - accuracy: 0.8298\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9092 - accuracy: 0.7225\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6014 - accuracy: 0.8243\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9051 - accuracy: 0.7249\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5725 - accuracy: 0.8302\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8918 - accuracy: 0.7288\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5526 - accuracy: 0.8345\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8882 - accuracy: 0.7271\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5770 - accuracy: 0.8339\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8983 - accuracy: 0.7242\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5614 - accuracy: 0.8334\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8756 - accuracy: 0.7376\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5608 - accuracy: 0.8302\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8753 - accuracy: 0.7304\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5671 - accuracy: 0.8344\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8937 - accuracy: 0.7276\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.8425\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8772 - accuracy: 0.7335\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.8373\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8678 - accuracy: 0.7345\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5779 - accuracy: 0.8266\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8710 - accuracy: 0.7346\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5538 - accuracy: 0.8371\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8624 - accuracy: 0.7393\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5445 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8537 - accuracy: 0.7396\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5533 - accuracy: 0.8400\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8825 - accuracy: 0.7296\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.8337\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8677 - accuracy: 0.7346\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5260 - accuracy: 0.8443\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8477 - accuracy: 0.7441\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.8406\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8585 - accuracy: 0.7411\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.8406\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8484 - accuracy: 0.7410\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5159 - accuracy: 0.8458\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8460 - accuracy: 0.7404\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5594 - accuracy: 0.8317\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8469 - accuracy: 0.7415\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 0.8436\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8457 - accuracy: 0.7418\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5188 - accuracy: 0.8439\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8306 - accuracy: 0.7493\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.8431\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8530 - accuracy: 0.7371\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5401 - accuracy: 0.8391\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8355 - accuracy: 0.7458\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5371 - accuracy: 0.8387\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8404 - accuracy: 0.7401\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5472 - accuracy: 0.8374\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8338 - accuracy: 0.7465\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5271 - accuracy: 0.8467\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8363 - accuracy: 0.7457\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.8386\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8215 - accuracy: 0.7487\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.8408\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8449 - accuracy: 0.7412\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.8389\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8404 - accuracy: 0.7403\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.8525\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8327 - accuracy: 0.7467\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5472 - accuracy: 0.8407\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8391 - accuracy: 0.7441\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5220 - accuracy: 0.8459\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8271 - accuracy: 0.7514\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5063 - accuracy: 0.8471\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8185 - accuracy: 0.7523\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5528 - accuracy: 0.8356\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8186 - accuracy: 0.7460\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5221 - accuracy: 0.8446\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3795 - accuracy: 0.5465\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1545 - accuracy: 0.6185\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2053 - accuracy: 0.6107\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0370 - accuracy: 0.6830\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.2757 - accuracy: 0.5788\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0360 - accuracy: 0.6705\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2659 - accuracy: 0.5912\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0392 - accuracy: 0.6680\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2693 - accuracy: 0.5986\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3349 - accuracy: 0.6216\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3299 - accuracy: 0.5682\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0320 - accuracy: 0.6763\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2361 - accuracy: 0.6125\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1444 - accuracy: 0.6252\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2793 - accuracy: 0.5934\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.9818 - accuracy: 0.7189\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4251 - accuracy: 0.5677\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2213 - accuracy: 0.6426\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2329 - accuracy: 0.6161\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0098 - accuracy: 0.6844\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3525 - accuracy: 0.5957\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0972 - accuracy: 0.6570\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3092 - accuracy: 0.5991\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1986 - accuracy: 0.6262\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3012 - accuracy: 0.6083\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0940 - accuracy: 0.6762\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.5092 - accuracy: 0.5568\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2692 - accuracy: 0.6357\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4915 - accuracy: 0.5761\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3443 - accuracy: 0.6010\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4548 - accuracy: 0.5817\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0148 - accuracy: 0.7051\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 1.4116 - accuracy: 0.5774\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0428 - accuracy: 0.6916\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 1.6589 - accuracy: 0.5346\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3998 - accuracy: 0.6189\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6289 - accuracy: 0.5608\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2465 - accuracy: 0.6362\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4935 - accuracy: 0.5844\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2403 - accuracy: 0.6450\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8337 - accuracy: 0.5209\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.5182 - accuracy: 0.6280\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5684 - accuracy: 0.5639\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0991 - accuracy: 0.6648\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5620 - accuracy: 0.5627\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2143 - accuracy: 0.6428\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6643 - accuracy: 0.5537\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4015 - accuracy: 0.6332\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7224 - accuracy: 0.5506\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1019 - accuracy: 0.6824\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9130 - accuracy: 0.5274\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3984 - accuracy: 0.6272\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3349 - accuracy: 0.6019\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0437 - accuracy: 0.6867\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9633 - accuracy: 0.5507\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0704 - accuracy: 0.7040\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8842 - accuracy: 0.5433\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4668 - accuracy: 0.6066\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.6861 - accuracy: 0.5578\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1465 - accuracy: 0.6795\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.6771 - accuracy: 0.5549\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0587 - accuracy: 0.6879\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.7497 - accuracy: 0.5584\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5473 - accuracy: 0.6144\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.6559 - accuracy: 0.5713\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2703 - accuracy: 0.6552\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9929 - accuracy: 0.5411\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3801 - accuracy: 0.6106\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1852 - accuracy: 0.5370\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5042 - accuracy: 0.6385\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0590 - accuracy: 0.5361\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6157 - accuracy: 0.6045\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7058 - accuracy: 0.5611\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2459 - accuracy: 0.6452\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7895 - accuracy: 0.5664\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6235 - accuracy: 0.6093\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7817 - accuracy: 0.5661\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3247 - accuracy: 0.6455\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9829 - accuracy: 0.5492\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3225 - accuracy: 0.6239\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.6831 - accuracy: 0.5053\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5937 - accuracy: 0.6580\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5072 - accuracy: 0.5239\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4952 - accuracy: 0.6263\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9590 - accuracy: 0.5536\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4089 - accuracy: 0.6148\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2418 - accuracy: 0.5320\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.2274 - accuracy: 0.5607\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0068 - accuracy: 0.5451\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1290 - accuracy: 0.6840\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6121 - accuracy: 0.5681\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.9789 - accuracy: 0.7099\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 2.5357 - accuracy: 0.5084\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4127 - accuracy: 0.6438\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3451 - accuracy: 0.5182\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7054 - accuracy: 0.6424\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9354 - accuracy: 0.7115\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7047 - accuracy: 0.7873\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9284 - accuracy: 0.7118\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7063 - accuracy: 0.7907\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9396 - accuracy: 0.7080\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6888 - accuracy: 0.7922\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8696 - accuracy: 0.7309\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6437 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8575 - accuracy: 0.7306\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6383 - accuracy: 0.8071\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8710 - accuracy: 0.7304\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6360 - accuracy: 0.8061\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8628 - accuracy: 0.7284\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6010 - accuracy: 0.8184\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8670 - accuracy: 0.7298\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6503 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8495 - accuracy: 0.7368\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6165 - accuracy: 0.8129\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8394 - accuracy: 0.7365\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6144 - accuracy: 0.8108\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8176 - accuracy: 0.7444\n",
            "782/782 [==============================] - 2s 1ms/step - loss: 0.6141 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8308 - accuracy: 0.7363\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6034 - accuracy: 0.8160\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8356 - accuracy: 0.7399\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5884 - accuracy: 0.8200\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8205 - accuracy: 0.7427\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6021 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8341 - accuracy: 0.7380\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5997 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8285 - accuracy: 0.7426\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5662 - accuracy: 0.8286\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8266 - accuracy: 0.7400\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6451 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8234 - accuracy: 0.7439\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5951 - accuracy: 0.8223\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8318 - accuracy: 0.7403\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5846 - accuracy: 0.8222\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8249 - accuracy: 0.7395\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6175 - accuracy: 0.8145\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8339 - accuracy: 0.7386\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5797 - accuracy: 0.8269\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8268 - accuracy: 0.7448\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5635 - accuracy: 0.8290\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8144 - accuracy: 0.7433\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6145 - accuracy: 0.8170\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8197 - accuracy: 0.7469\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5710 - accuracy: 0.8244\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8375 - accuracy: 0.7381\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5956 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8244 - accuracy: 0.7450\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6002 - accuracy: 0.8242\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8313 - accuracy: 0.7427\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5713 - accuracy: 0.8278\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8294 - accuracy: 0.7420\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5930 - accuracy: 0.8172\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8236 - accuracy: 0.7439\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6106 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8380 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.8228\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8439 - accuracy: 0.7408\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5851 - accuracy: 0.8211\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8301 - accuracy: 0.7403\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5951 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8264 - accuracy: 0.7460\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.8292\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8214 - accuracy: 0.7451\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5724 - accuracy: 0.8262\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8243 - accuracy: 0.7428\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6103 - accuracy: 0.8234\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8477 - accuracy: 0.7379\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5907 - accuracy: 0.8216\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8222 - accuracy: 0.7449\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5830 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8366 - accuracy: 0.7430\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5898 - accuracy: 0.8226\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8464 - accuracy: 0.7416\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5943 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8341 - accuracy: 0.7384\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5809 - accuracy: 0.8211\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8447 - accuracy: 0.7389\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6372 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8598 - accuracy: 0.7371\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5942 - accuracy: 0.8174\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8520 - accuracy: 0.7392\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.8179\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8345 - accuracy: 0.7446\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6297 - accuracy: 0.8099\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8458 - accuracy: 0.7415\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6355 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8576 - accuracy: 0.7414\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.8276\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8863 - accuracy: 0.7356\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6084 - accuracy: 0.8192\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8689 - accuracy: 0.7310\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.8243\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3265 - accuracy: 0.6244\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8012 - accuracy: 0.7634\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3368 - accuracy: 0.6183\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8254 - accuracy: 0.7577\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3290 - accuracy: 0.6203\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8044 - accuracy: 0.7634\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1873 - accuracy: 0.6660\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7436 - accuracy: 0.7798\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1821 - accuracy: 0.6533\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7723 - accuracy: 0.7704\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1819 - accuracy: 0.6649\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7534 - accuracy: 0.7770\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1227 - accuracy: 0.6620\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7218 - accuracy: 0.7861\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1259 - accuracy: 0.6726\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7366 - accuracy: 0.7836\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1198 - accuracy: 0.6708\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7310 - accuracy: 0.7823\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0999 - accuracy: 0.6806\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6977 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0939 - accuracy: 0.6741\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7249 - accuracy: 0.7874\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0833 - accuracy: 0.6793\n",
            "782/782 [==============================] - 2s 1ms/step - loss: 0.7061 - accuracy: 0.7900\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0763 - accuracy: 0.6759\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7003 - accuracy: 0.7936\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0482 - accuracy: 0.6901\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7149 - accuracy: 0.7870\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 1.0563 - accuracy: 0.6890\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.7996\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 1.0438 - accuracy: 0.6899\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6859 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0385 - accuracy: 0.6866\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6990 - accuracy: 0.7959\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0390 - accuracy: 0.6891\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6969 - accuracy: 0.7946\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0327 - accuracy: 0.6884\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6835 - accuracy: 0.7963\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0255 - accuracy: 0.6956\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7012 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0342 - accuracy: 0.6918\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6842 - accuracy: 0.7955\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0237 - accuracy: 0.6894\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6705 - accuracy: 0.8033\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0169 - accuracy: 0.6953\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6868 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0190 - accuracy: 0.6956\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6791 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0048 - accuracy: 0.6997\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6670 - accuracy: 0.8025\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9980 - accuracy: 0.7006\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6824 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0124 - accuracy: 0.6982\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0063 - accuracy: 0.6982\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0008 - accuracy: 0.6982\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7137 - accuracy: 0.7867\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0029 - accuracy: 0.6974\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6755 - accuracy: 0.8009\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9984 - accuracy: 0.7032\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6737 - accuracy: 0.7951\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9920 - accuracy: 0.7026\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9929 - accuracy: 0.6987\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6582 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0048 - accuracy: 0.6922\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6644 - accuracy: 0.8033\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9895 - accuracy: 0.7017\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6782 - accuracy: 0.8040\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9990 - accuracy: 0.6988\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6670 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0050 - accuracy: 0.6973\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6620 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9793 - accuracy: 0.7048\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6871 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0066 - accuracy: 0.6929\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.8079\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9876 - accuracy: 0.7042\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6532 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9793 - accuracy: 0.7050\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6784 - accuracy: 0.8033\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9880 - accuracy: 0.6994\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6691 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9896 - accuracy: 0.6980\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6643 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9664 - accuracy: 0.7057\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6700 - accuracy: 0.8049\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9865 - accuracy: 0.6968\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6816 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9806 - accuracy: 0.7004\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6611 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9768 - accuracy: 0.7023\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6832 - accuracy: 0.7964\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9893 - accuracy: 0.6981\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6663 - accuracy: 0.8004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 38.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 10s 4ms/step - loss: 0.7690 - accuracy: 0.7667\n",
            "Best: 0.8463733394940695 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.30802667140960693, Stdev: 0.02710307149426104 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.3052000006039937, Stdev: 0.030189877021222295 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.27453333139419556, Stdev: 0.015574338828098921 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.2595466673374176, Stdev: 0.013687808043055623 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.2768799960613251, Stdev: 0.022373152929795435 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2991333305835724, Stdev: 0.017808508878504544 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.29476000865300495, Stdev: 0.021049070249019173 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.23990666369597116, Stdev: 0.04321116844215839 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.289520005385081, Stdev: 0.022747802243065236 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.30527999997138977, Stdev: 0.02435538938690536 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.2890533407529195, Stdev: 0.02628257807398005 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.2985733250776927, Stdev: 0.037700294202023 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.3339466651280721, Stdev: 0.0058847043585306385 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.33213333288828534, Stdev: 0.06521147862279271 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.31019999583562213, Stdev: 0.06701550278207931 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.298799991607666, Stdev: 0.02769256306832255 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7887599865595499, Stdev: 0.0016329964712711142 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.7966799736022949, Stdev: 0.0045263084549302 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.7997600038846334, Stdev: 0.00029931960408833365 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8030533194541931, Stdev: 0.005592973146845032 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8040533264478048, Stdev: 0.0038649611630442926 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.80103999376297, Stdev: 0.003641536402658407 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8022133310635885, Stdev: 0.0021694345245377618 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8052133321762085, Stdev: 0.012379364553792772 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8032666643460592, Stdev: 0.004859667827088511 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8053733110427856, Stdev: 0.0032116962053170476 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8068533341089884, Stdev: 0.008174515082744081 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8029333154360453, Stdev: 0.000800213822126806 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.7986266613006592, Stdev: 0.0017736894670407647 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8037066658337911, Stdev: 0.0024410525441204856 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8049866557121277, Stdev: 0.007089838619746754 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8039599855740865, Stdev: 0.005950554067336837 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7919066548347473, Stdev: 0.0015292290073777037 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8144800066947937, Stdev: 0.0026631462879726274 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8194933334986368, Stdev: 0.0031123767714950597 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8237066864967346, Stdev: 0.002429671593529704 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8280933499336243, Stdev: 0.002672193347680131 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8339333335558573, Stdev: 0.0004736479520679701 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8357333342234293, Stdev: 0.005096016160578793 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8336800138155619, Stdev: 0.004978998889173434 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8370933334032694, Stdev: 0.0025849255802091217 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8418266773223877, Stdev: 0.0017350716147773687 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8403600056966146, Stdev: 0.0061984101354127545 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8420399824778239, Stdev: 0.002087942896627282 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8409600059191386, Stdev: 0.004106302827720235 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8394266565640768, Stdev: 0.000982150764306169 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8463733394940695, Stdev: 0.004844270557709703 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8424266775449117, Stdev: 0.004963248701835336 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.6573466857274374, Stdev: 0.02795413603371359 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.65529332558314, Stdev: 0.02406801559991882 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.662226657072703, Stdev: 0.040709895449124366 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6558666825294495, Stdev: 0.023806198876087096 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6376666824022929, Stdev: 0.030731123597721552 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6718933582305908, Stdev: 0.037859898940509944 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6363866726557413, Stdev: 0.006957784403882782 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6469199856122335, Stdev: 0.013241429107330657 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6654666662216187, Stdev: 0.02708675014327115 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6633599996566772, Stdev: 0.04135700343586405 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6524799863497416, Stdev: 0.03009047772826706 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6178666750590006, Stdev: 0.014791177345568068 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6333466569582621, Stdev: 0.017018087076181642 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6360533436139425, Stdev: 0.01552027710796487 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.6198133230209351, Stdev: 0.05045356158680531 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.6653466622034708, Stdev: 0.03149530855650493 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7900533278783163, Stdev: 0.0020249815788894285 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8061866760253906, Stdev: 0.0007224692262184104 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8104933102925619, Stdev: 0.007609765326055602 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8151999910672506, Stdev: 0.0033356048624096244 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8193333347638448, Stdev: 0.0018487766379312795 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8178266684214274, Stdev: 0.01107169290348177 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8211866617202759, Stdev: 0.0051127268976416005 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8234533270200094, Stdev: 0.004973144054003292 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8236266573270162, Stdev: 0.0037159619376232324 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8185999989509583, Stdev: 0.0030243324741562965 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8235200047492981, Stdev: 0.004000112991261935 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8237066864967346, Stdev: 0.0018904155519768825 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.819866677125295, Stdev: 0.002124220869810172 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8158266743024191, Stdev: 0.005077775214474613 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8108266592025757, Stdev: 0.0054268718275869475 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8237199982007345, Stdev: 0.0034392379231294526 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7614933451016744, Stdev: 0.0026966407080722403 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7757200002670288, Stdev: 0.003949418214473504 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.784000019232432, Stdev: 0.0015720789050724792 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7914800047874451, Stdev: 0.004017689889869146 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7934000094731649, Stdev: 0.005128850877857271 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7959466775258383, Stdev: 0.0011277478731737708 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7956933379173279, Stdev: 0.00041996479328139445 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8011599977811178, Stdev: 0.0015034158659117055 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8021066586176554, Stdev: 0.002840171511705086 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.794866661230723, Stdev: 0.005983129839880415 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7992400129636129, Stdev: 0.003285366334831995 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8027466734250387, Stdev: 0.0012659024534609458 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8029866615931193, Stdev: 0.003954036074718425 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8035866618156433, Stdev: 0.0022801870970286125 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8017200032869974, Stdev: 0.0031705373205732188 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.799893339474996, Stdev: 0.0026573780652483486 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgxux1GJU9pJ",
        "outputId": "5ad5276c-e209-4cdb-f275-28f96d2dafd6"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38.99983261028925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKoXPHiU9pJ",
        "outputId": "aa9b2487-ef73-4b3b-b1de-dbf69e70ae80"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 480}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7YfoSssU9pJ",
        "outputId": "8e9f5480-487f-44b0-d407-b593a6420877"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4944 - accuracy: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWUBmM5U9pJ",
        "outputId": "dc283e9d-1571-4d7c-807a-173bfd3ff78b"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8558400273323059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gptl39_LU9pJ"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "SIzhr2N4U9pJ"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnGCP5HEU9pJ"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UNVnH9PU9pK"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridserach experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66LuZ2FGU9pK"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}